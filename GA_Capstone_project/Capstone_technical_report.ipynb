{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project Technical Report\n",
    "\n",
    "By Kevin\n",
    "\n",
    "The title of this project is \"What should I watch next?\" with a sub title of: '(mis)Adventures in building a Recomendation Engine.\" Let's get started with the data set.\n",
    "\n",
    "https://grouplens.org/datasets/movielens/\n",
    "    \n",
    "    MovieLens 20M dataset.\n",
    "    \"Stable benchmark dataset. 20 million ratings and 465,000 tag applications applied to 27,000 movies by 138,000 users. Includes tag genome data with 12 million relevance scores across 1,100 tags. Released 4/2015; updated 10/2016 to update links.csv and add tag genome data.\"\n",
    "    \n",
    "It's the first link at the time of this writing (9/11/17). It's big... 800 MB unzipped so I'm not going to upload that to github. but the code works* (your milage may vary) so whoever is reading this can just unzip the files to where this notebook is located. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# more will be needed later\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So we have 3 main csv files (the others but are less important at this time). \n",
    "### genome_scores.csv\n",
    "This is in long format so pivot it where each row is an individual movie... and each row has 1128 columns. These columns are rated on a 0-5 scale for each movie. This is mostly how the content-filtering will be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genome = pd.read_csv('genome-scores.csv')\n",
    "genome = genome.pivot(index='movieId', columns='tagId', values='relevance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ratings.csv\n",
    "There is the ratings matrix. 138,493 users rated 26,744 movies, 20,000,263 times. It's in long format where each row is ratings. I don't recommend pivoting this unless you have a lot of RAM. (there also isn't a reason too). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratings = pd.read_csv('ratings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### movies.csv\n",
    "This has the movies. The movieID and genre information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movies = pd.read_csv('movies.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are missing data\n",
    "\n",
    "specifically on the genome. There are only 10,381 movies in the genome but there are ~27,000 movies that are rated. Getting that genome data would be difficult so we are just going to drop anything that doesn't tie into the genome. \n",
    "\n",
    "    (note: the movies we are dropping only have 1-2 ratings in the ratings dataframe. When we drop them, we will still have close to 20M ratings. Also: not having any genome data on these movies won't matter for collaborative-filtering... but simplicity ((and my sanity)) we are just going to drop them to combine content/collaborative filtering)\n",
    "    \n",
    "The fastest way to do this is to:\n",
    "1. Group the genome dataframe by movieID (if you havent pivoted yet)\n",
    "2. left join with the ratings dataframe. ratings(left) & genome(right)\n",
    "3. drop the null rows \n",
    "\n",
    "I did this on my laptop without problems in like 30 sec. (8 GB RAM, 4 cores 2.7 Ghz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I DID NOT PIVOT THIS GENOME BEFORE I DID THIS. so we'll just read it in again\n",
    "new_df = pd.read_csv('genome-scores.csv').groupby('movieId')[['movieId']].count()\n",
    "new_df.columns = ['dropmelater']\n",
    "new_df.reset_index(inplace=True)\n",
    "\n",
    "#now join them\n",
    "ratings_updated = ratings.join(new_df.set_index('movieId'), on='movieId', how='left', rsuffix='_drop_me_too')\n",
    "ratings_updated.dropna(axis=0, inplace=True)\n",
    "ratings_updated.drop('dropmelater', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end result is 19,800,443 rows in the ratings Dataframe. It also helps to save this too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratings_updated.to_csv('ratings_updated.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now to fix the movies Dataframe\n",
    "\n",
    "the genres column is a STRINGED up mess. So lets seperate them in to dummy variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movies = pd.read_csv('movies.csv')\n",
    "\n",
    "### https://stackoverflow.com/questions/18889588/create-dummies-from-column-with-multiple-values-in-pandas\n",
    "\n",
    "## This splits/dummies the strings in the genre column\n",
    "dummies = pd.get_dummies(movies_updated['genres'])\n",
    "\n",
    "atom_col = [c for c in dummies.columns if '|' not in c]\n",
    "\n",
    "for col in atom_col:\n",
    "    movies_updated[col] = dummies[[c for c in dummies.columns if col in c]].sum(axis=1)\n",
    "    \n",
    "movies_updated.drop('genres',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was one movie with \"no genres listed\" so I went to IMDB and manually entered the genre info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dont forget to save\n",
    "movies_updated.to_csv('movies_updated.csv', index=False)\n",
    "\n",
    "## When reading this into pandas again, the encoding changed. Don't ask my why, anyways, this fixes it.\n",
    "movies = pd.read_csv('movies_updated.csv', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you are still following along, make sure the movies and ratings dataframes are the upated ones we just made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember how I said there were 3 csv files?\n",
    "\n",
    "I lied. We are going to make a 4th 'users' dataframe and save that to a .csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users = ratings_updated.groupby('userId')[['userId']].count()\n",
    "users.columns = ['rated_count']\n",
    "users = users.join(ratings_updated.groupby('userId')[['rating']].mean())\n",
    "users.sort_values('rated_count', inplace=True)\n",
    "users.columns = ['rated_count', 'avg_rating']\n",
    "users.reset_index(inplace=True)\n",
    "\n",
    "# And save that to a CSV\n",
    "users.to_csv('users.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "We just preprocessed all of our data for modeling. We have 4 Dataframes we are constantly querrying (perfectly setup for SQL). \n",
    "\n",
    "1. Ratings\n",
    "2. Genome\n",
    "3. Movies\n",
    "4. Users\n",
    "\n",
    "\n",
    "I'm aware there are not outputs in the notebook and if you really want to see what they look like. you can find them here:\n",
    "https://github.com/kevinperlas/my_workspace/blob/master/MovieLense20M/MovieLens20M_EDA.ipynb\n",
    "\n",
    "\n",
    "Let's get modeling!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content Filtering\n",
    "\n",
    "Our data is already setup for modeling. We have a genome with 1000+ predictors. It's just a matter of puting the right data to fit a model. \n",
    "\n",
    "## Evaluation\n",
    "The idea is to see if we can make a model that could predict the \"next\" thing a user would like to see. To evaluate that model in an offline setting, we will take the latest movie a user rated as the test set, and all the previous movies as the training set. \n",
    "\n",
    "Over a bunch of users, we can compute the RMSE.\n",
    "\n",
    "We have 130,000+ users and fitting that many models is going to take a long time. Luckily, we already sorted our users dataframe by how often they rated movies so we can take a subset of that and get an equal distribution of active users.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_list = []\n",
    "for num in range(0, 138493, 100):\n",
    "    eval_list.append(num)\n",
    "evaluation_users = users.loc[eval_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are running regression models. we need to round the predicted values to the nearest .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def myround(x, prec=2, base=0.5):\n",
    "    return round(base * round(float(x)/base),prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate a model over a subset of our userbase. It takes some joining of our dataframes to fit the model so the following function will take a 'userId', get the appropriate data to about the user, fit the model, and predict the latest rating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def get_yhat(n): #Linear Regression, no Regularization\n",
    "    \n",
    "    # first step: get the movies that the user rated\n",
    "    user_n_df = ratings.loc[ratings['userId'] == n]\n",
    "    user_n_df = user_n_df.sort_values('timestamp')\n",
    "    \n",
    "    # Now join the movies to the genome\n",
    "    user_n_df = user_n_df.join(genome, on='movieId', how='left')\n",
    "    \n",
    "    # prep the df for modeling\n",
    "    user_n_df = user_n_df.set_index('movieId')\n",
    "    user_n_df.drop(['userId', 'timestamp'], axis=1, inplace=True)\n",
    "                   \n",
    "    #train test split\n",
    "    X= user_n_df.drop('rating', axis=1)\n",
    "    y= user_n_df[['rating']]\n",
    "                   \n",
    "    y_test = np.array(y.iloc[-1:]['rating'])\n",
    "    y_train = np.array(y.iloc[:-1]['rating'])\n",
    "    X_test = X.iloc[-1:]\n",
    "    X_train = X.iloc[:-1]\n",
    "    \n",
    "    #fit model\n",
    "    LR = LinearRegression()\n",
    "    LR.fit(X_train, y_train)\n",
    "    \n",
    "    #fix y_hat\n",
    "    y_hat = myround(LR.predict(X_test))\n",
    "    \n",
    "    if y_hat > 5:\n",
    "        y_hat = 5\n",
    "    \n",
    "    if y_hat < 0:\n",
    "        y_hat = 0\n",
    "    \n",
    "    #return adjusted prediction and y_test\n",
    "    return y_hat, y_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will get a list of the prediction (y_hat) and the actual (y_true)\n",
    "\n",
    "Note: I kept the index from the users dataframe in my evaluation_users dataframe. it's every 100 users index 0, so its 1385 users or 1% of the users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_hat_list = []\n",
    "y_true_list = []\n",
    "for num in range(0, 138500, 100): #it's 138500 so it will run the last one\n",
    "    y_hat, y_true = get_yhat(evaluation_users.loc[num]['userId'])\n",
    "    y_hat_list.append(y_hat)\n",
    "    y_true_list.append(y_true)\n",
    "    print(y_hat, y_true)\n",
    "    \n",
    "#my computer finished this in 3 min 45 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple Linear Regression was pretty fast and had a RMSE of: 1.0128239108395056\n",
    "\n",
    "If an app has to fit and predict ratings when a user clicks \"watch next\", Speed and adaptability also become very important (along with accuracy). \n",
    "\n",
    "A Baysian Ridge Regression is the option that makes the most sense since It 'learns' what regularization coefficent to use from the priors (ie. ratings history) from the specific user. It runs with the code above. Just instantiate BaysianRidge instead of a Linear Regression.\n",
    "\n",
    "This had a RMSE of : 0.9086428170370735\n",
    "An interesting point about using BaysianRidge is that the RMSE got smaller the when a user had rated more things (this makes sense intuitively since it has more priors). Segementing the Evaulation_users DF into thirds (still sorted by movies rated) the RMSE is as follows:\n",
    "\n",
    "    first: 0.9880235200593537\n",
    "    second: 0.9279931931832044\n",
    "    Third: 0.7997564564355761\n",
    "\n",
    "The Linear Regression did not follow this pattern and stayed at ~1 throughout the same sub-subsets. \n",
    "\n",
    "The only downside to the BaysianRidge is the computation time and requirement. The evaluation took 3x longer. But in the pipeline (coming soon), a user with very active history took about 3 seconds to fit and predict using a much larger test set in order to find the next highest rating they would like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## so how do we pipeline this?\n",
    "\n",
    "The idea is a user will choose a genre and the engine picks a movie for them. So this function takes a userId and genre and outputs the 5 movies with the highest predicted ratings. \n",
    "\n",
    "it's in \"dev mode\" right now looking at the next 5. For the app the bottom line will change to:\n",
    "```\n",
    "return movies_subset.iloc[0].merge(movies, left_on='movieId', right_on='movieId', how='left')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_recommendations(user_number, genre='Romance'):\n",
    "\n",
    "    #get user data\n",
    "    user_profile = ratings.loc[ratings['userId'] == user_number]\n",
    "    \n",
    "    #subset the movies by genre, take out the movies the user has seen\n",
    "    movies_subset =  movies.loc[movies[genre] == 1][['movieId']]\n",
    "    movies_subset = movies_subset.merge(user_profile, how='left', left_on='movieId', right_on='movieId')\n",
    "    movies_subset = movies_subset.loc[movies_subset['userId'].isnull()][['movieId']]\n",
    "    \n",
    "    ## now join with the genome\n",
    "    movies_subset = movies_subset.join(genome, on='movieId', how='left')\n",
    "    movies_subset.set_index('movieId', inplace=True)\n",
    "    \n",
    "    ## join the user profile with the genome and prepare for model fit\n",
    "    user_profile = user_profile.join(genome, on='movieId', how='left')\n",
    "    user_profile.set_index('movieId', inplace=True)\n",
    "    user_profile.drop(['userId', 'timestamp'], axis=1, inplace=True)\n",
    "    \n",
    "    #train the model\n",
    "    BR = BayesianRidge()\n",
    "    BR.fit(user_profile.drop('rating', axis=1), user_profile['rating'])\n",
    "    \n",
    "    #predict and get the y_hat\n",
    "    y_hat = BR.predict(movies_subset)\n",
    "    movies_subset['yhat'] = y_hat\n",
    "    movies_subset= movies_subset[['yhat']]\n",
    "    movies_subset.sort_values('yhat', ascending=False, inplace=True)\n",
    "    movies_subset.reset_index(inplace=True)\n",
    "    \n",
    "    #return top 5\n",
    "    movies_subset.head()\n",
    "    \n",
    "    return movies_subset.head().merge(movies, left_on='movieId', right_on='movieId', how='left')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "This is the Content-Fileterning model. It works! But it's limitations are that of Content-Filtering. Once a user build a certain profile of the movies they like. the model doesn't really leave that space. \n",
    "\n",
    "For example. userId: 3584 has favorably liked a lot of specific comedy movies. So when I ran that user through the pipeline with \"Romance\" as the genre, the output was ROMCOMs that make too much sense (There's something about Mary, the 40 Year old virgin, naked gun ((it counts)). Ran the same user through with \"Horror\" and \"Horror\" Comedies came up (Scary Movie, Shaun of the dead, Club Dredd). \n",
    "\n",
    "It's not as Serendipitous as I'd like it to be. So now we need to add Collaborative Filtering\n",
    "\n",
    "If you really want to see outputs, check out:\n",
    "https://github.com/kevinperlas/my_workspace/blob/master/MovieLense20M/Content_Modeling.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering\n",
    "\n",
    "Things are about to get expensive. both mentally and computationally. You are going to need to have AT LEAST 10 GB of RAM. Fitting and Evaluating this model I was running between 64-256 GB of RAM. \n",
    "\n",
    "And you're also going to need Surprise.  A python package for Collaborative Filtering https://github.com/NicolasHug/Surprise\n",
    "\n",
    "We are going to start with Evaluation of our model using user-based SVD. Default setting from the Surprise package.\n",
    "that is...\n",
    "\n",
    "    100 components dimensionality reduction\n",
    "    20 epochs to reduce the RMSE using Gradient Decent\n",
    "    Euclidean Distance for similarity measurement\n",
    "    \n",
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise import evaluate, print_perf\n",
    "from surprise.dataset import Reader\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### WARNING: This took over 3 hours to run on a m4.4XLarge - 16 cores, 64 GBs of RAM\n",
    "\n",
    "#Surprise reads directly fromt he CSV\n",
    "filepath = 'ratings_updated.csv'\n",
    "reader = Reader(line_format='user item rating timestamp', sep=',', skip_lines=1)\n",
    "\n",
    "# Dataset is a python object specific to Surprise\n",
    "data = Dataset.load_from_file(filepath, reader=reader)\n",
    "data.split(n_folds=5)\n",
    "\n",
    "perf = evaluate(algo, data, measures=['RMSE', 'MAE'])\n",
    "print_perf(perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output was:\n",
    "```\n",
    "        Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    \n",
    "RMSE    0.7851  0.7847  0.7844  0.7846  0.7847  0.7847  \n",
    "MAE     0.5974  0.5970  0.5968  0.5970  0.5970  0.5970  \n",
    "```\n",
    "\n",
    "The results are pretty good. Better than the content-filtering with the 5 folds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About here is where distributed computing would help \n",
    "Now we need to train the model on the whole set.\n",
    "\n",
    ".pkl the the model and run predictions with chunks of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### WARNING: NEED about 16 GB of RAM to run in about 30 min\n",
    "data = Dataset.load_from_file(filepath, reader=reader)\n",
    "trainset = data.build_full_trainset()\n",
    "\n",
    "# then pickel the file using the DUMP in surprise\n",
    "dump.dump('CF_full_fit.pkl', algo=algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RUNNING PREDICTIONS ON THE FULL SET WILL CRASH AN M4.16XLARGE INSTANCE\n",
    "after about 7 hours and $21\n",
    "\n",
    "The workaround is to predict the model 1% at a time. a system with 16 GB of RAM can predict a chunk in about 4 minutes. So this will take ~400 minutes on a single computer/instance to complete once the data is preprocessed. \n",
    "\n",
    "So, here's some code that splits the ratings matrix by users into 1% chunks\n",
    "\n",
    "\n",
    "or x100 .csv files for surprise to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Surprise can work with timestamps but dropping it made things run faster\n",
    "\n",
    "def get_ratings_subset(block):\n",
    "    \n",
    "    block.set_index('userId', inplace=True)\n",
    "    block = block[['rated_count']]\n",
    "    \n",
    "    new_df = ratings.join(block, on='userId', how='left')\n",
    "    new_df.dropna(inplace=True)\n",
    "    new_df.drop('rated_count', axis=1, inplace=True)\n",
    "    new_df.drop('timestamp', axis=1, inplace=True) \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    \n",
    "    temp_list = []\n",
    "    for num in range(i, 138493, 100):\n",
    "        temp_list.append(num)\n",
    "    \n",
    "    df = get_ratings_subset(users.loc[temp_list])\n",
    "    \n",
    "    filepath = 'Collab_sets/block' + str(i) + '.csv'\n",
    "    df.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will predict ALL the ratings for EVERY USER and EVERY MOVIE. But what we are interested in is the top predictions.\n",
    "\n",
    "In post deployment, these would be important to evaluate the RMSE in a live setting. I'm running out of hard drive space on my EC2 instance so I'm just saving the top 10.\n",
    "\n",
    "Users at most, have rated ~2000 movies. so there are predictions for the other 8000. These predictions as a whole can easily be saved in a pickled file for later evaluation and compliled to save hard drive space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top_n(predictions, n=10):\n",
    "    '''Return the top-N recommendation for each user from a set of predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        n(int): The number of recommendation to output for each user. Default\n",
    "            is 10.\n",
    "\n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    '''\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This line reads the fitted model from the .pkl\n",
    "__, loaded_algo = dump.load('CF_full_fit.pkl')\n",
    "\n",
    "\n",
    "\n",
    "for iteration in range(100):\n",
    "    #get the data\n",
    "    filename = 'Batches/' + 'block' + str(iteration) + '.csv' #I saved it in a file called 'batches/'\n",
    "    data = Dataset.load_from_file(filename, reader=reader)\n",
    "    \n",
    "    #temperary step to build the testset\n",
    "    _ = data.build_full_trainset()\n",
    "    \n",
    "    #this is the actual test set\n",
    "    testset = _.build_anti_testset()\n",
    "    \n",
    "    # Prections are happening now\n",
    "    predictions = loaded_algo.test(testset)\n",
    "    \n",
    "    # filters predictions to the top 10\n",
    "    print(iteration, \": Getting top 10\")\n",
    "    top_n = get_top_n(predictions, n=10)\n",
    "    \n",
    "    #convert to datafram and save to CSV\n",
    "    ### THIS IS VERY MESSY AND NEEDS CLEANING\n",
    "    df2 = pd.DataFrame().from_dict(top_n, orient='index')   \n",
    "    outputname = 'Batches/' +'output_' + 'block' + str(iteration) + '.csv'\n",
    "    df2.to_csv(outputname)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's it!\n",
    "\n",
    "Now all that's left is cleaning the output from collaborative filtering and some logic steps to determine which prediction (collaborative or content) takes priority. But this is a hybrid model of collaborative and content filtering.\n",
    "\n",
    "if you really want to see the outputs for these cells.\n",
    "\n",
    "Evaluation: https://github.com/kevinperlas/my_workspace/blob/master/MovieLense20M/Suprise_exploration.ipynb\n",
    "\n",
    "Splitting the test set into chunks: https://github.com/kevinperlas/my_workspace/blob/master/MovieLense20M/Collaborative%20test%20sets.ipynb\n",
    "\n",
    "Fitting, Predicting, pickle :  https://github.com/kevinperlas/my_workspace/blob/master/MovieLense20M/CF_100_testsets.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Everything that follows is still a WIP and will probably have notes that I've set up for myself to read. \n",
    "\n",
    "This block of code cleans the output of collaborative filtering so it can be used in the Recommender engine notebook. The output is a new CSV of the top 10 recommendations using Collaborative-Filtering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# instantiate a DF\n",
    "combineDF = pd.read_csv('output_block0.csv')\n",
    "\n",
    "#Read every csv form the chunks and combine them into one big DF\n",
    "for n in range(100):\n",
    "    filename = 'output_block' + str(n) + '.csv'\n",
    "    temp_df = pd.read_csv(filename)\n",
    "    combinedDF = pd.concat([combinedDF, temp_df])\n",
    "\n",
    "#rename the columns\n",
    "combinedDF.columns = ['userId', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "#the ratings look like tuples but they are stings now. lets just grab the movieId since it's already in top 10 order\n",
    "## we aren't going to actually use this function but it helps to remember just what I'm actually trying to do.\n",
    "### for me anyways\n",
    "\n",
    "def just_movieID(long_string):\n",
    "    return int(long_string.split(',')[0][2:-1])\n",
    "\n",
    "#This actually splits the string\n",
    "for n in range(1, 10):\n",
    "    combinedDF[str(n)] = combinedDF[str(n)].apply(lambda x: int(x.split(',')[0][2:-1]))\n",
    "\n",
    "#and save it to a CSV\n",
    "combinedDF.to_csv('CF_top10.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Combiner\n",
    "\n",
    "this is where things just turn into a logic puzzle and there really isn't a good/easy way to test its accuracy in an offilne setting.\n",
    "\n",
    "So we have two different predictive ratings on movies that may or may not be the same. The content model will provide things are in line with what the user has already seen. The Collaborative results has greater potential to be serendipitous. And the collaborative also scored higher in its RMSE (offline which is important to remember since optimizing for an offline RMSE will lead to overfitting). \n",
    "\n",
    "_the block is reserved for the Hybrid combiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test out how fast a single userId can get predictions\n",
    "\n",
    "keep in mind that even if the predictions could be made in real time. the model we trained will be outdated. It could still be useful/relevant... plus we have the content filtering that can be trained in real-time to keep collaborative results in-line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
